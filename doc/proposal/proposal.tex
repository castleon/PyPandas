\documentclass[sigconf]{acmart}

\usepackage{booktabs} % For formal tables

\begin{document}
\title{PyPandas: A scalable data cleaning library}

\author{Pei-Lun Liao}
\affiliation{%
  \institution{New York University}
}
\email{pll273@nyu.edu}

\author{Chia-Hsien Lin}
\affiliation{%
  \institution{New York University}
}
\email{chl566@nyu.edu}

\author{Shang-Hung Tsai}
\affiliation{%
  \institution{New York University}
}
\email{st3127@nyu.edu}

\begin{abstract}
Data cleaning become a challenge\cite{datacleaning} in data mining and machine learning tasks. In this paper, we propose PyPandas, a scalable library running on Spark\cite{spark} to solve data cleaning tasks. PyPandas provides basic cleaning features such as missing values handling, outlier detection and data scaling. Morevoer, advanced features like text cleaning are supported as well.   

\end{abstract}

\maketitle

\section{Introduction}
In recent years, as storage devices become cheaper and cheaper, storing big data in thousands of computers is easier than before. People start to think how to obtain big value from the huge dataset. The term, Big Data, was created to describe this phenomenon\cite{bigdata}. As the result, data mining\cite{Han, MMD} and machine learning\cite{ml} get popular nowadays. The quality of data is one of the key points to mine good value in the huge dataset\cite{dataquality}. Hence, data cleaning become a challenge in the first step of data management and analysis\cite{datacleaning, DBS-045, PW}.
	
To process huge dataset efficiently, distributed systems and parallel computing frameworks\cite{mapreduce, gfs, hadoop} are introduced. Spark\cite{spark} is one of the most popular open source projects for industry and academia. It is built on Hadoop\cite{hadoop} and provides a way to manage distributed memory. PySpark\cite{pyspark} is an extended library for Python programmers.
	
In the Python machine learning and data mining ecosystem, Pandas\cite{pandas} is the most popular data management library. Pandas dataframe provides a way to collect data and the data can be transformed from Python primitive data structures or Numpy\cite{numpy} array easily. Moreover, Pandas provides several data management methods such as missing value filling, data indexing and data profiling. 
	
However, Pandas can only be used in a single machine and could not be scaled to manage huge dataset. In this paper, we propose PyPandas, a library built on PySpark, which support basic data management features, missing value handling or duplicated data removing. Moreover, we add common data processing features into our library to provide user friendly usage such as outlier detection and numerical data scaling. These features are important for the machine learning tasks and are supported in Scikit-Learn\cite{scikit-learn}, a mainstream machine learning library, as well.
	
Furthermore, text data is another common data type in our storage. Text data are hard to process because of multiple languages, newly invented word, typo, abbreviation, url and emoji etc. Although we have NLTK\cite{nltk}, a popular text cleaning library, there is no scalable text cleaning library. In this paper, we also provide tools for users to clean text easily. 
	
The features of our library is summarized below.
\begin{itemize}
	\item{Saclable data management library runs on Spark}
	\item{Missing value handling, duplicated data removing, outlier detection and numerical data scaling}
	\item{Text processing such as url detection, punctuation removing, pattern searching and replacement. }
\end{itemize}	

\section{Related Work}
There exists a number of open-source library for data cleaning and parallel data computing. 
\begin{itemize}
	\item{Optimus}\cite{optimus} is a framework that can perform distributed data cleaning and preprocessing. This framework works well with Spark and its DataFrame can scale in big cluster. Optimus comes with helpful tools such as removing special characters and replacing null values.
	\item{Dask}\cite{dask} is another open-source library that supports parallel analytic computing. It provides scalable parallel data structures that extend interfaces like pandas. At the same time, it offers low latency and high responsiveness.
	\item{SparklingPandas}\cite{sparklingpandas} attempts to combine the power of spark and pandas to scale data analysis. It provides a Pandas-like API that is built using Spark's DataFrame class. Unfortunately, it only support spark v1.4 and Python 2.7, and its development has ended.
\end{itemize}



\section{Proposed Methods}
In this project, we prepare to build our library on top of the PySpark\cite{pyspark, spark}. We will handle RDD data structure and implement useful data cleaning fucntions in our library. The ways to do that are collecting common cleaning functions into library and customizing map functions to do advanced cleaning task such as pattern searching and replacement with regular expression. We will survey the Pandas\cite{pandas} and Scikit-Learn\cite{scikit-learn} libraries to find out key features and create Pandas-liked and Scikit-Learn-liked API to let users start with easily.

\section{Experiment}
\subsection{Dataset}
Dataset 1: 311 Service Requests from 2010 to Present
\begin{itemize}
	\item{Summary: All 311 Service Requests from 2010 to present.}
	\item{Source URL: https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9}
	\item{Data property: text is majority}
	\item{Rows: 9,063,486, Columns: 53}
	\item{Size: 6.01G}
	\item{Missing value: There are about 15 out of 53 columns with null value which is about 28.3\%}
	\item{Reason to choose the data: In this dataset, most of the data is consisted of alphabetical words which is one of the data property we are looking for. Besides, the column named "Resolution Description" contains the description about the service status notes which are all alphabetical words we need. }
\end{itemize}
Dataset 2: Statistical Summary Period Attendance Reporting (PAR)
\begin{itemize}
	\item{Summary: Statistical report on attendance by borough, grade. Alternate views of same data by grade level and enrollment (register). All students including YABC, adults, LYFE babies and charters, home instruction, home/hospital, CBO UPK.}
	\item{Source URL: https://data.cityofnewyork.us/Education/Statistical-Summary-Period-Attendance-Reporting-PA/hrsu-3w2q}
	\item{Data property: number is majority}
	\item{Rows: 24,421, Columns: 39}
	\item{Size: 2.8Mb}
	\item{Missing value: There are about 5 out of 39 columns with null value which is about 13\%}
	\item{Reason to choose the data: In this dataset, most of the data is consisted of numbers which is also one of the data property we are looking for.}
\end{itemize}
Dataset 3: DOB Job Application Filings
\begin{itemize}
	\item{Summary: A list of job applications filed for a particular day and associated data. Prior weekly and monthly reports are archived at DOB and are not available on NYC Open Data.}
	\item{Source URL: https://data.cityofnewyork.us/Housing-Development
	/DOB-Job-Application-Filings/ic3t-wcy2}
	\item{Data Property: categorical dataset}
	\item{Rows: 5,260,437, Columns: 89}
	\item{Size: 2.88G}
	\item{Missing Value: There are about 36 out of 89 columns with null value which is about 40\%.}
	\item{Reason to choose the data: In this dataset, there are 89 columns which satisfies the categorical property we need in this project.}
\end{itemize}
\subsection{Evaluation}
We will evaluate our project by comparing it against other existing open-source libraries, including Optimus\cite{optimus}, Dask\cite{dask}, and SparkingPandas\cite{sparklingpandas}. 
\subsubsection{Features}
We will compare our data cleaning features with Optimus, particularly those frequently used functionality in data science such as null value handling, special character removal, duplicate detection, etc. 
\subsubsection{Performance}
We will evaluate the performance of our library by measuring the time and space consumed and drawing comparison with existing libraries. This performance evaluation will show that this library can be useful in production. 
\subsubsection{Userbility}
We might assess the usability of our library by doing a survey among a random group of users. We will compare the installation process and APIs provided and generate scores of user experience.

\section{Conclusions}
Due to the lack of useful scalable data cleaning library on Spark, we propose PySpark, a scalable data cleaning library. We provide basic data cleaning features such as missing value handling, duplicated data removing and data scaling. Moreover, our library supports text cleaning feature with regular expression.

\section{reference}
\bibliographystyle{ACM-Reference-Format}
\bibliography{citation}


\end{document}
