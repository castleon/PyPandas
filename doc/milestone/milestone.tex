\documentclass[sigconf]{acmart}

\usepackage{booktabs} % For formal tables

\begin{document}
\title{SparkCleaner: A data cleaning toolkits for Spark}

\author{Pei-Lun Liao}
\affiliation{%
  \institution{New York University}
}
\email{pll273@nyu.edu}

\author{Chia-Hsien Lin}
\affiliation{%
  \institution{New York University}
}
\email{chl566@nyu.edu}

\author{Shang-Hung Tsai}
\affiliation{%
  \institution{New York University}
}
\email{st3127@nyu.edu}

\begin{abstract}
Data cleaning becomes a challenge in data mining and machine learning tasks. In this paper, we propose SparkCleaner, a data cleaning toolkits running on Spark, to provide further solutions to data cleaning tasks. SparkCleaner provides advance cleaning features such as outlier detection, data scaling and text pattern searching and replacement.   
\end{abstract}

\maketitle

\section{Introduction}
In recent years, as storage devices become cheaper, storing big data in thousands of computers is easier than before. People start thinking about how to extract useful information from the huge datasets. The term, Big Data, was created to describe this phenomenon\cite{bigdata}. As the result, data mining\cite{Han, MMD} and machine learning\cite{ml} get popular nowadays. The quality of data is one of the key factors to successful data mining\cite{dataquality}. Hence, data cleaning becomes a challenge in the first step of data management and analysis\cite{datacleaning, DBS-045, PW}.
	
To process huge dataset efficiently, distributed systems and parallel computing frameworks\cite{mapreduce, gfs, hadoop} were introduced. Spark\cite{spark} is one of the most popular open source projects for industry and academia. It is built on Hadoop\cite{hadoop} and provides a way to manage distributed memory. PySpark\cite{pyspark} is an extended library for Python programmers to work with Spark.
	
In the Python machine learning and data mining ecosystem, Pandas\cite{pandas} is the most popular data management library. Pandas dataframe provides convenient ways to collect data and several data management methods for missing value filling, data indexing and data profiling. In PySpark SQL module, PySpark provides Pandas-liked DataFrame to handle data indexing, data intergation, and simple data cleaning tasks such as missing value handling and duplication removing.
	
However, PySpark DataFrame does not support advance data processing features like outlier detection and data scaling. Although in PySpark ML module, we have several clustering and scaling algorithms, the intergation with PySpark DataFrame is not helpful. User has to go back and forth to handle their dataset.  In this paper, we propose SparkCleaner, a data cleaning toolkits built on top of PySpark, that intergrates PySpark ML module to supports user friendly data processing features such as outlier detection and numerical data scaling. These features are important for the machine learning tasks and are supported in Scikit-Learn\cite{scikit-learn}, a mainstream machine learning library, as well.
	
Furthermore, text data is another common data type in our storage. Text data is hard to process because of different languages, newly invented words, typos, abbreviations, urls and emoji etc. Although we have NLTK\cite{nltk}, a popular text cleaning library, there is no intergation with PySpark DataFrame. In this paper, we also intergate NLTK within our toolkits and provide customized tools for users to clean text easily. 
	
The features of our library are summarized below.
\begin{itemize}
	\item{User friendly data cleaning toolkits built on PySpark}
	\item{Outlier detection and numerical data scaling}
	\item{Text processing such as url detection, punctuation removing, pattern searching and replacement. }
\end{itemize}	

\section{Related Work}
There exists a number of open-source library for data cleaning and parallel data computing. 
\begin{itemize}
	\item{Optimus}\cite{optimus} is a framework that can perform distributed data cleaning and preprocessing. This framework works well with Spark and its DataFrame can scale in big cluster. Optimus comes with helpful tools such as removing special characters and replacing null values.
	\item{Dask}\cite{dask} is another open-source library that supports parallel analytic computing. It provides scalable parallel data structures that extend interfaces like Pandas. At the same time, it offers low latency and high responsiveness.
	\item{SparklingPandas}\cite{sparklingpandas} attempts to combine the power of Spark and Pandas to scale data analysis. It provides a Pandas-like API that is built using Spark's DataFrame class. Unfortunately, it only support spark v1.4 and Python 2.7, and its development has ended.
\end{itemize}

\section{Problem Formulation}
In this paper, we focus on three major tasks, outlier detection, data scaling, and text pattern searching and replacement.
\subsection{Outlier Detection}
Outlier or anomalies detection is a challenging task in the filed of data cleaning\cite{anomal}. In this paper, we simply define outlier as the data that is far away from data centroid. Hence, given a dataset $X$, a clustering algorithm $A$\cite{MMD}, a distance measurement $d$, and the user-defined threshold $t$, the outlier is simply defined as following.

\[ outlier = \{x | x \in X, d( x, x_{A_X(x)} ) > t \}  \]

where $A_X(x)$ cluster $x$ into a cluster given the dataset $X$ and $x_c$  is the centroid of cluster $c$.

\subsection{Scaling and Normalization}
Scaling and normalization are common preprocess in data cleaning. It is crucial to several machine learning models or optimization methods. Scaling increases the convergence speed and chance to find good optimal\cite{convex}. The scaling problem could be formulated as below.
Given an one dimensional numberical dataset $X$, we provide a scaling method $S$ such that scaled elements are bouned in same reasonable range $[a, b]$.

\[ S_X(x) \in [a, b]\] 

where $x \in X$ and $|a - b|$ should not be large.

To define the problem formally, we consider and implement several specific scaling methods.
\begin{itemize}
	\item{Standard scale}
		\[ S(X) = \dfrac{X-\mu}{\sigma} \in \text{proper deviation range}\]
		where $\mu$ is the mean of $X$ and $\sigma$ is the standard deviation of $X$
	\item{Min-Max scale}
		\[ S(X) = \dfrac{X-min(X)}{max(X) - min(X)}\in [0.0, 1.0] \]
	\item{Max-Abs scale}
		\[ S(X) = \dfrac{X}{max(X)} \in [-1.0, 1.0] \]
	\item{P-norm Normalization}
		\[ S(X) = \dfrac{X}{\|X\|_{p}} \in [-1.0, 1.0]\]
\end{itemize}

\subsection{Pattern Searching and Replacment}
Pattern searching and replacement is a common technique in cleaning text data. Given a string $t$ and a regular expression\cite{theory_com} $re$, we replace the matched pattern with a user-defined string $s$. The replacement function $R$ could be defined as following.
Consider $t$ as a concatenation of non-overlaped substrings $t = t_1...t_n$.

\[ R(t) =  r(t_1)...r(t_i)...r(t_n)\]

\[r(t) = \begin{cases} s & \text{if $M$ accept $t$} \\ t & \text{otherwise} \end{cases}\]

\[\text{such that $M$ does not accept any substring of } R(t)\]

where $M$ is the generalized nondeterministic finite automaton\cite{theory_com} corrsponding to the regular expression $re$. We do not define replacement order and simply ignore it.


\section{Methods, Architecture and Design}
In this project, we build our library on top of PySpark. We handle the DataFrame data structure and implement useful data cleaning fucntions in our library. The ways to do that are intergrating PySpark ML module and customizing user define functions to perform advanced cleaning features such as pattern searching and replacement with regular expression. We survey the PySpark\cite{pyspark}, Pandas\cite{pandas} and Scikit-Learn\cite{scikit-learn} libraries to find out key features and create Pandas-like and Scikit-Learn-like API to provide easy usage and seamless adaptation.

\subsection{Outlier Detection}
\subsubsection{Design} 
%Outliers affects the experiment results and can lead to false conclusions.
Outlier detection is a common problem in data cleaning. Our goal in outlier detection is to intergrate the existing clustering algorithms in PySpark ML module to provide user-friendly solution. Hence, we will let user be able to select the existng clustering algorithms to detect outliers. Also, users are able to choose their threshold to indentify outliers. To help users pick the proper threshold, we provide statistics summary before choosing one. Possible future work of outlier detection is providing framework to register user defined clustering algorithms or implementing popular clustering algorithms e.g. DBScan\cite{DBScan} for our library.

\subsubsection{Architecture}
The implementation of the outlier detection functionality integrates the PySpark ml package. Our architecture hides the complexity of training and tuning clustering models, while it exposes easily accessible APIs for users to remove outliers. In addition, our data cleaning library implements User Defined Functions to compute distance between data points and cluster centers, as well as other summary statistics. At this time, our current remover can handle outliers with kMeans algorithm. We will refact our architecture to give its ability to intergrate all existing clusting algorithm in PySpark ML module. To simplify our architecture, we will try Factory Design Pattern to construct different kinds of outlier removers with the same API.

\subsubsection{Methods}
\begin{itemize}
	\item{KMeans}\cite{KMeans} Users can specify the number of clusters to use based their knowledge about the dataset. The system will choose random initial values, and perform 20 iterations of updates. Finally, a summary of the clusterings will be generated, which includes cluster centers, cluster sizes, distances, etc. User can then filter out outliers in each cluster by specifying a minimum distance from the cluster center, and all data points that are beyond that distance will be removed from the dataset.
\end{itemize}

\subsection{Scaling and Normalization}
\subsubsection{Design} 
Standardization of datasets is a common requirement for many machine learning algorithms, since objective functions will not work properly without scaling or normalization. Therefore, we provide several useful scaling functions and normalizing function. These functions can help standardize the  specified columns and improve the performance of learning algorithms.

\subsubsection{Methods}
\begin{itemize}
	\item{The standard\_scale()} : Performs basic scaling on a particular column or a list of columns in a dataframe so that the values have unit standard deviation and/or zero mean. 
	\item{The min\_max\_scale()}: Rescales columns to a user\-defined range(e.g. [0, 1]) using min max scaling.
	\item{The max\_abs\_scale()}: Transforms columns to range between -1 and 1 by dividing through the maximum absolute value in the columns. This operation does not shift/center the data, and thus does not destroy any sparsity. 
	\item{The normalize()}: Given a list of columns, the normalize function can generate a normalized feature vector having unit norm. The default p-norm value for normalization is 2.0, yet users can optionally specify the p-norm value.
\end{itemize}

\subsubsection{Architecture}
The implementation of the scaling and normalization functionality integrates the PySpark ml package as well. We include three the most commonly used scaling functions and one normalize function. The architecture and the API design is based on scikit-learn, the most popular machine learning library for the Python, so it is intuitive and easy to use. 

\subsection{Pattern Searching and Replacement}
\subsubsection{Design} 
\subsubsection{Methods}
\subsubsection{Architecture}

\section{Experiment}
\subsection{Dataset}
Three datasets are chosen for the experiments. Each of them has different properties and can be used to test different aspects of the library. The summary of datasets could be found in table \ref{tab:dataset}.
\begin{itemize}
	\item{311 Service Requests}\cite{nycopendata1} This dataset contains all 311 Service Requests since 2010. The majority of data has string type, including both categorical  data (e.g. City, Status) and variable length data (e.g. Address, Description). This can be useful in testing the string-related features of the library.
	\item{DOB Permit Issuance}\cite{nycopendata2} The dataset consists of a list of permits for buildings in NYC issued by Department of Building. It has diverse data types, including categorical, string, numerical, and geographical data. Hence, it provides a good test case for the versatility of the library. 
	\item{DOB Job Application Filings}\cite{nycopendata3} The dataset stores job applications filed through the Borough Offices in NYC. There are many columns containing numerical data, such as fee and number of stories. It can be used to test the data scaling and outlier detection features. 
\end{itemize}

\begin{table}
\caption{Selected dataset}   
\label{tab:dataset}
\begin{tabular}{lccc}   
 		                 & (Row, Col)  & Size      & Missing Value  \\  
\hline
 311 Service Requests   & (9M, 53)     & 6.01 GB  & 28\%            \\ 
 Permit Issuance            & (3M, 60)     & 1.43 GB  & 20\%             \\  
 Job Application Filings   & (5M, 89)     & 2.88 GB  & 40\%             \\ 


\end{tabular}   
\end{table}

\subsection{Evaluation}
We will evaluate our project by comparing it against other existing open-source libraries, including Optimus\cite{optimus}, Dask\cite{dask}, and SparkingPandas\cite{sparklingpandas}. 
\subsubsection{Features}
We will compare our data cleaning features with previous works, particularly those frequently used functionality in data science such as null value handling, special character removal, duplicate detection, etc. 
\subsubsection{Performance}
We will evaluate the performance of our library by measuring the time and space consumed and drawing comparison with existing libraries. This performance evaluation will show that this library is efficient and can be useful in production. 
\subsubsection{Usability}
We might assess the usability of our library by doing a survey among a random group of users. We will compare the installation process and APIs provided and generate scores of user experience.

\subsection{Result}
In the experiments on scaling and normalization functions, we chose three columns in each dataset as input to do scaling, normalization and time for each experiment. The experiment results can be found in table \ref{tab:Experiment result}.

\begin{table}
\caption{Experiment result}   
\label{tab:Experiment result}
\begin{tabular}{lccc}   
 	                      &  311 Service  &  Permit    &  Job Application \\ &Requests&Issuance&Filings   \\  
\hline
standard\_scale()   & 1 m 51 s     & 48.36 s & 41.58 s            \\ 
min\_max\_scale()  &  1 m 50 s   & 48.74 s  & 41.69 s             \\  
max\_abs\_scale()  & 1 m 50 s     &  47.27 s  & 41.52 s             \\ 
normalize()           &  1.8 s     &  1.2 s & 1.1 s             \\ 

\end{tabular}   
\end{table}

\section{Conclusions}
Due to the lack of useful scalable data cleaning library on Spark, we propose PySpark, a scalable data cleaning library. We provide basic data cleaning features such as missing value handling, duplicated data removing and data scaling. Moreover, our library supports text cleaning feature with regular expression.

%\section{reference}
\bibliographystyle{ACM-Reference-Format}
\bibliography{citation}


\end{document}
